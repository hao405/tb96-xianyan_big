# DDP å¤šGPUè®­ç»ƒ - å¿«é€Ÿå¼€å§‹

## ğŸ¯ é—®é¢˜è§£å†³

åŸå§‹é”™è¯¯ï¼š
```
RuntimeError: Trying to access a forward AD level with an invalid index
TypeError: cannot unpack non-iterable NoneType object
```

**æ ¹æœ¬åŸå› **ï¼š`DataParallel` ä¸ `functorch` çš„ `jacfwd`/`vmap` ä¸å…¼å®¹

**è§£å†³æ–¹æ¡ˆ**ï¼šè¿ç§»åˆ° `DistributedDataParallel (DDP)`

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. ç›´æ¥è¿è¡Œ traffic.shï¼ˆæœ€ç®€å•ï¼‰

```bash
cd /Users/zhuhao/experiment/first_learning/TB-96/TimeBridge-v3-xianyan
bash scripts/traffic.sh
```

### 2. æˆ–ä½¿ç”¨é€šç”¨è„šæœ¬

```bash
bash scripts/run_ddp_tune.sh
```

### 3. æµ‹è¯• DDP è®¾ç½®ï¼ˆæ¨èå…ˆè¿è¡Œï¼‰

```bash
bash scripts/test_ddp.sh
```

## ğŸ“‹ å·²ä¿®æ”¹çš„æ–‡ä»¶

1. âœ… `experiments/exp_basic.py` - æ·»åŠ  DDP device ç®¡ç†
2. âœ… `experiments/exp_long_term_forecasting.py` - ä½¿ç”¨ DDP æ›¿ä»£ DataParallel  
3. âœ… `data_provider/data_factory.py` - æ·»åŠ  DistributedSampler
4. âœ… `tune_big.py` - æ·»åŠ  DDP åˆå§‹åŒ–å’Œæ¸…ç†
5. âœ… `scripts/traffic.sh` - ä½¿ç”¨ torchrun å¯åŠ¨
6. âœ… `model/TimeBridge.py` - ä¿æŒåŸæ ·ï¼ˆDDP ä¸‹æ— éœ€ä¿®æ”¹ï¼‰

## ğŸ”§ å…³é”®å˜åŒ–

### ä¹‹å‰ï¼ˆDataParallelï¼‰
```bash
python -u tune_big.py --use_multi_gpu --devices 0,1,2,3
```

### ç°åœ¨ï¼ˆDDPï¼‰
```bash
torchrun --nproc_per_node=4 --master_port=29500 tune_big.py --use_multi_gpu --devices 0,1,2,3
```

## âš™ï¸ é‡è¦å‚æ•°

| å‚æ•° | è¯´æ˜ | ç¤ºä¾‹ |
|------|------|------|
| `--nproc_per_node` | æ¯ä¸ªèŠ‚ç‚¹çš„GPUæ•°é‡ | `8` |
| `--master_port` | ä¸»è¿›ç¨‹é€šä¿¡ç«¯å£ | `29500` |
| `--use_multi_gpu` | å¯ç”¨å¤šGPU | å·²é»˜è®¤å¼€å¯ |
| `--devices` | GPUåˆ—è¡¨ | `0,1,2,3,4,5,6,7` |
| `--batch_size` | å•GPUæ‰¹æ¬¡å¤§å° | `4`ï¼ˆ8GPUæ—¶æœ‰æ•ˆæ‰¹æ¬¡=32ï¼‰ |

## ğŸ® AMD ROCm ç‰¹å®šè®¾ç½®

```bash
# æŒ‡å®šä½¿ç”¨çš„ GPU
export HIP_VISIBLE_DEVICES=0,1,2,3,4,5,6,7

# ç¦ç”¨ MIOpen ç¼“å­˜
export MIOPEN_DISABLE_CACHE=1
export MIOPEN_SYSTEM_DB_PATH=""
```

è¿™äº›å·²åœ¨ `traffic.sh` ä¸­è‡ªåŠ¨è®¾ç½®ã€‚

## ğŸ“Š æ‰¹æ¬¡å¤§å°è®¡ç®—

**æœ‰æ•ˆæ‰¹æ¬¡å¤§å° = batch_size Ã— GPUæ•°é‡**

ç¤ºä¾‹ï¼š
- `--batch_size 4` Ã— 8ä¸ªGPU = **æœ‰æ•ˆæ‰¹æ¬¡ 32**
- `--batch_size 8` Ã— 4ä¸ªGPU = **æœ‰æ•ˆæ‰¹æ¬¡ 32**

æ ¹æ®æ˜¾å­˜è°ƒæ•´ `batch_size`ã€‚

## âœ… éªŒè¯ DDP æ˜¯å¦å·¥ä½œ

è¿è¡Œååº”è¯¥çœ‹åˆ°ï¼š
```
Use GPU (DDP): cuda:0, Rank: 0/8
Use GPU (DDP): cuda:1, Rank: 1/8
...
Use GPU (DDP): cuda:7, Rank: 7/8
```

æ£€æŸ¥æ‰€æœ‰ GPU æ˜¯å¦éƒ½åœ¨å·¥ä½œï¼š
```bash
watch -n 1 rocm-smi
```

## ğŸ› å¸¸è§é—®é¢˜

### ç«¯å£å ç”¨
```bash
# æ›´æ”¹ç«¯å£
torchrun --master_port=29501 ...
```

### å†…å­˜ä¸è¶³
```python
# å‡å° batch_size
--batch_size 2  # è€Œä¸æ˜¯ 4
```

### NCCL é”™è¯¯
```bash
# ä½¿ç”¨ gloo backendï¼ˆè¾ƒæ…¢ä½†æ›´ç¨³å®šï¼‰
export TORCH_DISTRIBUTED_BACKEND=gloo
```

## ğŸ“š è¯¦ç»†æ–‡æ¡£

æŸ¥çœ‹ `DDP_GUIDE.md` è·å–å®Œæ•´æ–‡æ¡£ã€‚

## ğŸ‰ å®Œæˆï¼

ç°åœ¨å¯ä»¥ä½¿ç”¨å¤šGPUè®­ç»ƒäº†ï¼Œä¸ä¼šå†å‡ºç° functorch é”™è¯¯ï¼
